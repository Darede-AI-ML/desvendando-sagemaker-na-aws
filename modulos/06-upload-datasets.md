# MÃ³dulo 6: Upload de Datasets para o Ambiente SageMaker

## Objetivos de Aprendizagem

Ao final deste mÃ³dulo, vocÃª serÃ¡ capaz de:
- Compreender conceitos fundamentais do Amazon S3
- Criar e configurar buckets S3 para ML
- Fazer upload de dados por mÃºltiplos mÃ©todos
- Organizar datasets seguindo boas prÃ¡ticas
- Acessar dados do S3 dentro do SageMaker
- Otimizar performance de leitura/escrita

## DuraÃ§Ã£o Estimada
40 minutos

---

## 1. IntroduÃ§Ã£o ao Amazon S3

### O que Ã© Amazon S3?

Amazon Simple Storage Service (S3) Ã© um serviÃ§o de armazenamento de objetos que oferece:

- ğŸ“¦ **Armazenamento ilimitado** escalÃ¡vel
- ğŸ”’ **Durabilidade** de 99.999999999% (11 noves)
- ğŸŒ **Disponibilidade** global
- ğŸ’° **Custo baixo** ($0.023/GB-mÃªs em eu-central-1)
- ğŸš€ **Alta performance** para ML workloads

### Conceitos Fundamentais

```mermaid
graph TB
    AWS[AWS Account]
    Region[RegiÃ£o eu-central-1]
    
    Bucket1[Bucket: projeto-dados]
    Bucket2[Bucket: projeto-outputs]
    
    Folder1[Prefixo: raw/]
    Folder2[Prefixo: processed/]
    
    Object1[Objeto: dataset.csv]
    Object2[Objeto: train.parquet]
    Object3[Objeto: test.parquet]
    
    AWS --> Region
    Region --> Bucket1
    Region --> Bucket2
    
    Bucket1 --> Folder1
    Bucket1 --> Folder2
    
    Folder1 --> Object1
    Folder2 --> Object2
    Folder2 --> Object3
    
    style Bucket1 fill:#99ccff
    style Bucket2 fill:#99ccff
    style Folder1 fill:#99ff99
    style Folder2 fill:#99ff99
    style Object1 fill:#ffeb99
    style Object2 fill:#ffeb99
    style Object3 fill:#ffeb99
```

**Hierarquia:**
- **Bucket**: Container principal (como um "drive")
- **Prefixo**: Simula pastas (ex: `raw/`, `processed/`)
- **Objeto**: Arquivo individual (ex: `dataset.csv`)
- **Key**: Caminho completo do objeto (ex: `raw/dataset.csv`)

### Por que S3 para Machine Learning?

âœ… **Vantagens:**
- IntegraÃ§Ã£o nativa com SageMaker
- Escala automaticamente
- Suporta datasets de qualquer tamanho
- Versionamento de dados
- Compartilhamento fÃ¡cil entre equipes
- Backup e disaster recovery

---

## 2. Criando Buckets S3

### Nomenclatura de Buckets

**Regras:**
- âœ… Ãšnico globalmente (em toda AWS)
- âœ… 3-63 caracteres
- âœ… Apenas letras minÃºsculas, nÃºmeros, hÃ­fens
- âŒ NÃ£o pode comeÃ§ar com hÃ­fen
- âŒ NÃ£o pode ter caracteres especiais

**PadrÃ£o recomendado:**
```
[projeto]-[ambiente]-[propÃ³sito]-[account-id]-[regiÃ£o]

Exemplos:
sagemaker-training-data-123456789012-eu-central-1
sagemaker-training-outputs-123456789012-eu-central-1
```

### MÃ©todo 1: Via Console AWS

**Passo 1: Acessar S3**
1. Console AWS â†’ Buscar "S3"
2. Clique em **"Create bucket"**

**Passo 2: ConfiguraÃ§Ãµes BÃ¡sicas**
```
Bucket name: sagemaker-training-data-[SEU-ACCOUNT-ID]-eu-central-1
AWS Region: Europe (Frankfurt) eu-central-1
```

**Passo 3: ConfiguraÃ§Ãµes de Bloqueio**
```
Block all public access: âœ“ (RECOMENDADO)
```

**Passo 4: Versionamento**
```
Bucket Versioning: Enable (recomendado para dados importantes)
```

**Passo 5: Criptografia**
```
Default encryption: 
  âœ“ Server-side encryption with Amazon S3 managed keys (SSE-S3)
```

**Passo 6: Criar**
- Revise e clique em **"Create bucket"**

### MÃ©todo 2: Via AWS CLI

```bash
# Obter account ID
ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)

# Criar bucket
aws s3 mb s3://sagemaker-training-data-${ACCOUNT_ID}-eu-central-1 \
  --region eu-central-1

# Habilitar versionamento
aws s3api put-bucket-versioning \
  --bucket sagemaker-training-data-${ACCOUNT_ID}-eu-central-1 \
  --versioning-configuration Status=Enabled

# Habilitar criptografia
aws s3api put-bucket-encryption \
  --bucket sagemaker-training-data-${ACCOUNT_ID}-eu-central-1 \
  --server-side-encryption-configuration '{
    "Rules": [{
      "ApplyServerSideEncryptionByDefault": {
        "SSEAlgorithm": "AES256"
      }
    }]
  }'

# Bloquear acesso pÃºblico
aws s3api put-public-access-block \
  --bucket sagemaker-training-data-${ACCOUNT_ID}-eu-central-1 \
  --public-access-block-configuration \
    "BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=true,RestrictPublicBuckets=true"
```

### MÃ©todo 3: Via CloudFormation

Use o template fornecido:

```bash
aws cloudformation create-stack \
  --stack-name sagemaker-s3-buckets \
  --template-body file://cloudformation/s3-bucket.yaml \
  --parameters \
    ParameterKey=ProjectName,ParameterValue=sagemaker-training \
    ParameterKey=EnvironmentName,ParameterValue=training \
  --region eu-central-1
```

Este template cria:
- âœ… Bucket de dados
- âœ… Bucket de outputs
- âœ… Bucket de logs
- âœ… PolÃ­ticas de seguranÃ§a
- âœ… Lifecycle rules

---

## 3. Organizando Datasets

### Estrutura Recomendada

```
s3://sagemaker-training-data-xxxxx/
â”œâ”€â”€ raw/                      # Dados originais (imutÃ¡veis)
â”‚   â”œâ”€â”€ dataset_v1.csv
â”‚   â””â”€â”€ dataset_v2.csv
â”‚
â”œâ”€â”€ processed/                # Dados processados
â”‚   â”œâ”€â”€ clean_data.parquet
â”‚   â””â”€â”€ features.parquet
â”‚
â”œâ”€â”€ train/                    # Dados de treinamento
â”‚   â”œâ”€â”€ train.csv
â”‚   â””â”€â”€ train_large.parquet
â”‚
â”œâ”€â”€ validation/               # Dados de validaÃ§Ã£o
â”‚   â””â”€â”€ val.csv
â”‚
â”œâ”€â”€ test/                     # Dados de teste
â”‚   â””â”€â”€ test.csv
â”‚
â”œâ”€â”€ external/                 # Dados externos
â”‚   â””â”€â”€ reference_data.json
â”‚
â””â”€â”€ metadata/                 # Metadados
    â”œâ”€â”€ schema.json
    â””â”€â”€ data_dictionary.md
```

### Boas PrÃ¡ticas de Nomenclatura

**Arquivos:**
```
âœ… Bom:
train_2026-02-05.csv
customer_features_v2.parquet
model_output_20260205_153045.json

âŒ Ruim:
dados.csv
final_final_v2_real.xlsx
output (1).txt
```

**ConvenÃ§Ãµes:**
- Use underscores `_` em vez de espaÃ§os
- Inclua datas no formato ISO: `YYYY-MM-DD`
- Use versÃµes explÃ­citas: `_v1`, `_v2`
- Seja descritivo mas conciso

---

## 4. Upload de Dados

### MÃ©todo 1: Via Console AWS (GUI)

**Para arquivos pequenos (<160GB):**

1. Acesse o bucket no console S3
2. Clique em **"Upload"**
3. Arraste arquivos ou clique em **"Add files"**
4. **(Opcional)** Selecione pasta de destino
5. Clique em **"Upload"**

**Vantagens:**
- âœ… Simples e visual
- âœ… NÃ£o requer configuraÃ§Ã£o

**Desvantagens:**
- âŒ Lento para muitos arquivos
- âŒ Limite de tamanho
- âŒ NÃ£o automatizÃ¡vel

### MÃ©todo 2: Via AWS CLI

#### Upload Simples

```bash
# Upload de um arquivo
aws s3 cp dataset.csv s3://seu-bucket/raw/ \
  --region eu-central-1

# Upload de uma pasta inteira
aws s3 cp ./data/ s3://seu-bucket/raw/ \
  --recursive \
  --region eu-central-1

# Upload com progresso
aws s3 cp dataset.csv s3://seu-bucket/raw/ \
  --region eu-central-1 \
  --no-progress  # ou --progress para ver progresso
```

#### Upload com Sync (SincronizaÃ§Ã£o)

```bash
# Sincronizar pasta local com S3 (sÃ³ upload de novos/modificados)
aws s3 sync ./local-data/ s3://seu-bucket/raw/ \
  --region eu-central-1

# Sync com exclusÃµes (deleta no S3 o que nÃ£o existe localmente)
aws s3 sync ./local-data/ s3://seu-bucket/raw/ \
  --delete \
  --region eu-central-1

# Sync com filtros
aws s3 sync ./local-data/ s3://seu-bucket/raw/ \
  --exclude "*" \
  --include "*.csv" \
  --include "*.parquet" \
  --region eu-central-1
```

#### Upload com Metadados

```bash
aws s3 cp dataset.csv s3://seu-bucket/raw/ \
  --metadata "version=1,date=2026-02-05,owner=joao" \
  --content-type "text/csv" \
  --region eu-central-1
```

### MÃ©todo 3: Via Python (boto3)

#### Upload BÃ¡sico

```python
import boto3
from pathlib import Path

# Cliente S3
s3 = boto3.client('s3', region_name='eu-central-1')

bucket_name = 'sagemaker-training-data-xxxxx-eu-central-1'

# Upload de arquivo Ãºnico
s3.upload_file(
    Filename='dataset.csv',
    Bucket=bucket_name,
    Key='raw/dataset.csv'
)

print("Upload concluÃ­do!")
```

#### Upload com Progress Bar

```python
import boto3
from boto3.s3.transfer import TransferConfig
from tqdm import tqdm

class ProgressPercentage:
    def __init__(self, filename):
        self._filename = filename
        self._size = Path(filename).stat().st_size
        self._seen_so_far = 0
        self._pbar = tqdm(total=self._size, unit='B', unit_scale=True)

    def __call__(self, bytes_amount):
        self._seen_so_far += bytes_amount
        self._pbar.update(bytes_amount)

# Upload com progresso
s3 = boto3.client('s3')
s3.upload_file(
    'large_dataset.csv',
    bucket_name,
    'raw/large_dataset.csv',
    Callback=ProgressPercentage('large_dataset.csv')
)
```

#### Upload de MÃºltiplos Arquivos

```python
import boto3
from pathlib import Path

s3 = boto3.client('s3')
bucket_name = 'seu-bucket'

# Upload de todos CSV em uma pasta
data_dir = Path('./data')
csv_files = data_dir.glob('*.csv')

for csv_file in csv_files:
    s3_key = f'raw/{csv_file.name}'
    print(f"Uploading {csv_file} to s3://{bucket_name}/{s3_key}")
    
    s3.upload_file(
        str(csv_file),
        bucket_name,
        s3_key
    )

print("Todos os arquivos foram enviados!")
```

#### Upload com Multipart (Arquivos Grandes)

```python
import boto3
from boto3.s3.transfer import TransferConfig

# ConfiguraÃ§Ã£o para arquivos grandes
config = TransferConfig(
    multipart_threshold=1024 * 25,  # 25 MB
    max_concurrency=10,
    multipart_chunksize=1024 * 25,
    use_threads=True
)

s3 = boto3.client('s3')

# Upload de arquivo grande
s3.upload_file(
    'very_large_dataset.parquet',
    bucket_name,
    'raw/very_large_dataset.parquet',
    Config=config
)
```

### MÃ©todo 4: Direto do SageMaker Studio

#### Via Terminal no Studio

```bash
# No terminal do SageMaker Studio
cd ~/work/data

# Upload via AWS CLI
aws s3 cp dataset.csv s3://seu-bucket/raw/

# Upload de pasta
aws s3 sync . s3://seu-bucket/raw/local-backup/
```

#### Via Notebook

```python
import sagemaker
import boto3

# SessÃ£o SageMaker (jÃ¡ autenticado)
session = sagemaker.Session()
bucket = session.default_bucket()  # Bucket padrÃ£o do SageMaker

# Upload via SageMaker SDK
input_data = session.upload_data(
    path='./data/train.csv',
    bucket=bucket,
    key_prefix='datasets/train'
)

print(f"Data uploaded to: {input_data}")
# Output: s3://sagemaker-eu-central-1-123456789012/datasets/train/train.csv
```

---

## 5. Acessando Dados do S3

### MÃ©todo 1: Download Completo

```python
import boto3

s3 = boto3.client('s3')

# Download de arquivo
s3.download_file(
    Bucket='seu-bucket',
    Key='raw/dataset.csv',
    Filename='./local_dataset.csv'
)
```

### MÃ©todo 2: Leitura Direta com Pandas

```python
import pandas as pd
import boto3

# OpÃ§Ã£o 1: Via S3 URI
s3_uri = 's3://seu-bucket/raw/dataset.csv'
df = pd.read_csv(s3_uri)

# OpÃ§Ã£o 2: Via boto3
s3 = boto3.client('s3')
obj = s3.get_object(Bucket='seu-bucket', Key='raw/dataset.csv')
df = pd.read_csv(obj['Body'])

# Para Parquet (mais eficiente!)
df = pd.read_parquet('s3://seu-bucket/processed/data.parquet')
```

### MÃ©todo 3: Leitura com S3Fs (Recomendado)

```python
import pandas as pd
import s3fs

# Filesystem S3
fs = s3fs.S3FileSystem(anon=False)

# Listar arquivos
files = fs.ls('seu-bucket/raw/')
print(files)

# Ler CSV
with fs.open('seu-bucket/raw/dataset.csv', 'rb') as f:
    df = pd.read_csv(f)

# Ler mÃºltiplos Parquet (particionado)
df = pd.read_parquet(
    's3://seu-bucket/processed/',
    engine='pyarrow'
)
```

### MÃ©todo 4: Leitura Otimizada para ML

```python
import sagemaker

session = sagemaker.Session()

# Input para Training Job
train_input = sagemaker.inputs.TrainingInput(
    s3_data='s3://seu-bucket/train/',
    content_type='text/csv'
)

# Ou para leitura local otimizada
from sagemaker.s3 import S3Downloader

S3Downloader.download(
    s3_uri='s3://seu-bucket/raw/',
    local_path='./data/',
    sagemaker_session=session
)
```

---

## 6. Verificando e Listando Dados

### Via AWS CLI

```bash
# Listar conteÃºdo do bucket
aws s3 ls s3://seu-bucket/ --region eu-central-1

# Listar recursivamente
aws s3 ls s3://seu-bucket/raw/ --recursive --human-readable

# Contar arquivos
aws s3 ls s3://seu-bucket/raw/ --recursive | wc -l

# Calcular tamanho total
aws s3 ls s3://seu-bucket/raw/ --recursive --summarize

# Buscar arquivos especÃ­ficos
aws s3 ls s3://seu-bucket/ --recursive | grep ".csv"
```

### Via Python

```python
import boto3

s3 = boto3.client('s3')
bucket_name = 'seu-bucket'

# Listar objetos
response = s3.list_objects_v2(
    Bucket=bucket_name,
    Prefix='raw/'
)

print(f"Total de objetos: {response['KeyCount']}")

for obj in response.get('Contents', []):
    print(f"{obj['Key']} - {obj['Size']} bytes - {obj['LastModified']}")

# Listar todos (paginaÃ§Ã£o automÃ¡tica)
import boto3

s3_resource = boto3.resource('s3')
bucket = s3_resource.Bucket(bucket_name)

for obj in bucket.objects.filter(Prefix='raw/'):
    print(f"{obj.key} - {obj.size / 1024 / 1024:.2f} MB")
```

---

## 7. Boas PrÃ¡ticas

### Performance

#### Use Formatos Otimizados

```python
# âŒ CSV: Lento, nÃ£o otimizado
df.to_csv('s3://bucket/data.csv')

# âœ… Parquet: RÃ¡pido, colunar, comprimido
df.to_parquet('s3://bucket/data.parquet')

# âœ… Feather: Ainda mais rÃ¡pido para colunas
df.to_feather('s3://bucket/data.feather')
```

**ComparaÃ§Ã£o de performance:**
| Formato | Tamanho | Tempo Escrita | Tempo Leitura |
|---------|---------|---------------|---------------|
| CSV | 100% | 10s | 15s |
| Parquet | 30% | 3s | 2s |
| Feather | 40% | 1s | 1s |

#### Particione Dados Grandes

```python
# Particionar por data
df.to_parquet(
    's3://bucket/processed/',
    partition_cols=['year', 'month'],
    engine='pyarrow'
)

# Estrutura resultante:
# processed/
#   â”œâ”€â”€ year=2026/
#   â”‚   â”œâ”€â”€ month=01/
#   â”‚   â”‚   â””â”€â”€ data.parquet
#   â”‚   â””â”€â”€ month=02/
#   â”‚       â””â”€â”€ data.parquet
```

### SeguranÃ§a

```python
# âœ… Sempre use criptografia
s3.upload_file(
    'sensitive_data.csv',
    bucket_name,
    'raw/sensitive_data.csv',
    ExtraArgs={'ServerSideEncryption': 'AES256'}
)

# âœ… Use presigned URLs para compartilhamento temporÃ¡rio
url = s3.generate_presigned_url(
    'get_object',
    Params={'Bucket': bucket_name, 'Key': 'raw/dataset.csv'},
    ExpiresIn=3600  # 1 hora
)
```

### Custos

```python
# âœ… Use Intelligent-Tiering para dados raramente acessados
s3.put_object(
    Bucket=bucket_name,
    Key='archive/old_data.csv',
    Body=data,
    StorageClass='INTELLIGENT_TIERING'
)

# âœ… Delete dados temporÃ¡rios
s3.delete_object(Bucket=bucket_name, Key='temp/intermediate.csv')
```

---

## 8. Troubleshooting

### Erro: "Access Denied"

**Causa:** PermissÃµes insuficientes

**SoluÃ§Ã£o:**
```bash
# Verificar polÃ­ticas do bucket
aws s3api get-bucket-policy --bucket seu-bucket

# Verificar ACL
aws s3api get-bucket-acl --bucket seu-bucket

# Verificar IAM role do SageMaker
aws iam get-role --role-name SageMakerExecutionRole
```

### Erro: "NoSuchBucket"

**Causa:** Bucket nÃ£o existe ou regiÃ£o errada

**SoluÃ§Ã£o:**
```bash
# Verificar se bucket existe
aws s3 ls s3://seu-bucket/ --region eu-central-1

# Listar todos os buckets
aws s3 ls

# Verificar regiÃ£o do bucket
aws s3api get-bucket-location --bucket seu-bucket
```

### Upload muito lento

**Causa:** Rede lenta ou arquivo muito grande

**SoluÃ§Ã£o:**
```python
# Use multipart upload
from boto3.s3.transfer import TransferConfig

config = TransferConfig(
    multipart_threshold=1024 * 25,  # 25 MB
    max_concurrency=10
)

s3.upload_file('large.csv', bucket, 'key', Config=config)
```

---

## 9. Checklist de ValidaÃ§Ã£o

- [ ] Criei bucket S3 com nomenclatura adequada
- [ ] Configurei criptografia e bloqueio de acesso pÃºblico
- [ ] Organizei dados em estrutura de pastas lÃ³gica
- [ ] Fiz upload de dados de teste com sucesso
- [ ] Consigo listar e acessar dados do S3
- [ ] Li dados do S3 em um notebook SageMaker
- [ ] Entendo as boas prÃ¡ticas de organizaÃ§Ã£o

---

## 10. Recursos Adicionais

### DocumentaÃ§Ã£o
- [Amazon S3 User Guide](https://docs.aws.amazon.com/s3/)
- [S3 Best Practices](https://docs.aws.amazon.com/AmazonS3/latest/userguide/best-practices.html)
- [SageMaker with S3](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-work-with-data.html)

---

## PrÃ³ximo MÃ³dulo

Dados prontos! Vamos executar cÃ³digo de exemplo!

â¡ï¸ [MÃ³dulo 7: ExecuÃ§Ã£o de CÃ³digo de Exemplo](07-execucao-codigo-exemplo.md)
